{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Recurrent Neural Network\n",
    "\n",
    "The general architecture of the model is based on [this tutorial](https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07).\n",
    "\n",
    "The data we'll be using is a sample of jobs corresponding to the same group according to the [Standard Occupational Classification](https://www.bls.gov/soc/) . The data is in a tsv file and can be downloaded [here](https://www.onetcenter.org/dl_files/database/db_20_1_text/Sample%20of%20Reported%20Titles.txt).\n",
    "\n",
    "https://sorenbouma.github.io/blog/oneshot/\n",
    "https://deepmind.com/research/publications/one-shot-learning-memory-augmented-neural-networks/\n",
    "https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O*NET-SOC Code</th>\n",
       "      <th>Reported Job Title</th>\n",
       "      <th>Shown in My Next Move</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11-1011.00</td>\n",
       "      <td>Chief Diversity Officer (CDO)</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11-1011.00</td>\n",
       "      <td>Chief Executive Officer (CEO)</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-1011.00</td>\n",
       "      <td>Chief Financial Officer (CFO)</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11-1011.00</td>\n",
       "      <td>Chief Nursing Officer</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11-1011.00</td>\n",
       "      <td>Chief Operating Officer (COO)</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  O*NET-SOC Code             Reported Job Title Shown in My Next Move\n",
       "0     11-1011.00  Chief Diversity Officer (CDO)                     N\n",
       "1     11-1011.00  Chief Executive Officer (CEO)                     Y\n",
       "2     11-1011.00  Chief Financial Officer (CFO)                     Y\n",
       "3     11-1011.00          Chief Nursing Officer                     N\n",
       "4     11-1011.00  Chief Operating Officer (COO)                     N"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "file_url = 'https://www.onetcenter.org/dl_files/database/db_20_1_text/Sample%20of%20Reported%20Titles.txt'\n",
    "csv = StringIO(requests.get(file_url).text)\n",
    "df = pd.read_csv(csv, sep='\\t').head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset\n",
    "\n",
    "Let's create positive samples with pairs of job titles corresponding to the same SOC, and negative examples with pairs of job titles sampled from different SOC codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_left</th>\n",
       "      <th>job_right</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33164</th>\n",
       "      <td>Industrial Rehabilitation Consultant</td>\n",
       "      <td>Staff Occupational Therapist</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12708</th>\n",
       "      <td>Manufacturing Director</td>\n",
       "      <td>Top Coater</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41398</th>\n",
       "      <td>Lifeguard</td>\n",
       "      <td>Water Safety Instructor (WSI)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70360</th>\n",
       "      <td>Issuing Operator</td>\n",
       "      <td>Stock Preparation Operator (Stock Prep Operator)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21491</th>\n",
       "      <td>Certified Shorthand Reporter (CSR)</td>\n",
       "      <td>Deposition Reporter</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   job_left  \\\n",
       "33164  Industrial Rehabilitation Consultant   \n",
       "12708                Manufacturing Director   \n",
       "41398                             Lifeguard   \n",
       "70360                      Issuing Operator   \n",
       "21491    Certified Shorthand Reporter (CSR)   \n",
       "\n",
       "                                              job_right  target  \n",
       "33164                      Staff Occupational Therapist     1.0  \n",
       "12708                                        Top Coater     0.0  \n",
       "41398                     Water Safety Instructor (WSI)     1.0  \n",
       "70360  Stock Preparation Operator (Stock Prep Operator)     1.0  \n",
       "21491                               Deposition Reporter     1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "jobs_left = []\n",
    "jobs_right = []\n",
    "target = []\n",
    "\n",
    "soc_codes = df['O*NET-SOC Code'].unique()\n",
    "for code in soc_codes:\n",
    "    similar_jobs = df[df['O*NET-SOC Code'] == code]['Reported Job Title']\n",
    "    positive_pairs = list(itertools.combinations(similar_jobs, 2))\n",
    "    jobs_left.extend([p[0] for p in positive_pairs])\n",
    "    jobs_right.extend([p[1] for p in positive_pairs])\n",
    "    target.extend([1.]*len(positive_pairs))\n",
    "    \n",
    "    other_jobs = df[df['O*NET-SOC Code'] != code]['Reported Job Title']\n",
    "    for i in range(len(positive_pairs)):\n",
    "        jobs_left.append(np.random.choice(similar_jobs))\n",
    "        jobs_right.append(np.random.choice(other_jobs))\n",
    "        target.append(0.)\n",
    "\n",
    "dataset = pd.DataFrame({\n",
    "        'job_left': jobs_left,\n",
    "        'job_right': jobs_right,\n",
    "        'target': target\n",
    "    }).sample(frac=1)  # Shuffle dataset\n",
    "\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_val = train_test_split(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('functiontransformer', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function preprocess_job_titles at 0x117690ae8>,\n",
       "          inv_kw_args=None, inverse_func=None, kw_args=None,\n",
       "          pass_y='deprecated', validate=False)), ('textstosequences', TextsToSequences()), ('padder', Padder(max_length=7))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.pipeline import make_pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from zeugma import TextsToSequences, Padder, ItemSelector, EmbeddingTransformer\n",
    "\n",
    "maxlen = 7\n",
    "vocab_size = 10000\n",
    "\n",
    "def preprocess_job_titles(job_titles):\n",
    "    \"\"\" Return a list of clean job titles \"\"\"\n",
    "    def preprocess_job_title(raw_job_title):\n",
    "        \"\"\" Clean a single job title\"\"\"\n",
    "        job_title = re.sub(r'\\(.*\\)', '', raw_job_title)\n",
    "        return job_title.lower().strip()\n",
    "    return [preprocess_job_title(jt) for jt in job_titles]\n",
    "    \n",
    "pipeline = make_pipeline(\n",
    "    FunctionTransformer(preprocess_job_titles, validate=False),\n",
    "    TextsToSequences(num_words=vocab_size), \n",
    "    Padder(max_length=maxlen),\n",
    ")\n",
    "\n",
    "pipeline.fit(list(df_train['job_left'])+list(df_train['job_right']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_left_train = pipeline.transform(df_train['job_left'])\n",
    "X_right_train = pipeline.transform(df_train['job_right'])\n",
    "seq_train = [X_left_train, X_right_train]\n",
    "\n",
    "y_train = df_train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate, Flatten\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "\n",
    "glove = EmbeddingTransformer('glove')\n",
    "EMBEDDING_DIM = glove.model.get('the').shape[0]\n",
    "\n",
    "word_index = pipeline.get_params()['textstosequences'].word_index\n",
    "\n",
    "def create_embedding_matrix(vocab_size, word_index, embedding_dim=EMBEDDING_DIM):\n",
    "    \"\"\" Prepare embedding matrix \"\"\"\n",
    "    num_words = min(vocab_size, len(word_index))\n",
    "    embedding_matrix = np.zeros((num_words+1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= vocab_size:\n",
    "            continue\n",
    "        embedding_vector = glove.model.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(vocab_size, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "left_input (InputLayer)         (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_input (InputLayer)        (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 128)          1176396     left_input[0][0]                 \n",
      "                                                                 right_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 128)          0           sequential_3[1][0]               \n",
      "                                                                 sequential_3[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "masltsm_distance (Lambda)       (None, 1)            0           subtract[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,176,396\n",
      "Trainable params: 285,696\n",
      "Non-trainable params: 890,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Lambda, Subtract, Merge\n",
    "from keras import backend as K\n",
    "\n",
    "def exponent_neg_manhattan_distance(arms_difference):\n",
    "    return K.exp(-K.sum(K.abs(arms_difference), axis=1, keepdims=True))\n",
    "\n",
    "def siamese_lstm(maxlen=maxlen):\n",
    "    \"\"\" Define, compile and return a siamese LSTM model \"\"\"\n",
    "    input_shape = (maxlen,)\n",
    "    left_input = Input(input_shape, name='left_input')\n",
    "    right_input = Input(input_shape, name='right_input')\n",
    "\n",
    "    # load pre-trained word embeddings into an Embedding layer\n",
    "    # note that we set trainable = False so as to keep the embeddings fixed\n",
    "    embedding_layer = Embedding(len(embedding_matrix),\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=maxlen,\n",
    "                                trainable=False,\n",
    "                                name='embeddings')\n",
    "\n",
    "    seq = Sequential()\n",
    "    seq.add(embedding_layer)\n",
    "    seq.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2,\n",
    "                               return_sequences=True)))\n",
    "    seq.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2,)))\n",
    "    \n",
    "    left_output = seq(left_input)\n",
    "    right_output = seq(right_input)\n",
    "\n",
    "    subtracted = Subtract(name='subtract')([left_output, right_output])\n",
    "    malstm_distance = Lambda(exponent_neg_manhattan_distance, \n",
    "                             name='masltsm_distance')(subtracted)\n",
    "\n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=malstm_distance)\n",
    "\n",
    "    siamese_net.compile(loss=\"binary_crossentropy\", optimizer='adam',\n",
    "                        metrics=['accuracy'])\n",
    "    return siamese_net\n",
    "\n",
    "siamese_lstm = siamese_lstm()\n",
    "\n",
    "siamese_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 46272 samples, validate on 11569 samples\n",
      "Epoch 1/10\n",
      "46272/46272 [==============================] - 85s 2ms/step - loss: 0.6106 - acc: 0.6966 - val_loss: 0.4742 - val_acc: 0.8025\n",
      "Epoch 2/10\n",
      "46272/46272 [==============================] - 95s 2ms/step - loss: 0.4770 - acc: 0.7988 - val_loss: 0.4331 - val_acc: 0.8303\n",
      "Epoch 3/10\n",
      "46272/46272 [==============================] - 77s 2ms/step - loss: 0.4432 - acc: 0.8232 - val_loss: 0.4126 - val_acc: 0.8424\n",
      "Epoch 4/10\n",
      "46272/46272 [==============================] - 73s 2ms/step - loss: 0.4191 - acc: 0.8415 - val_loss: 0.3925 - val_acc: 0.8553\n",
      "Epoch 5/10\n",
      "46272/46272 [==============================] - 80s 2ms/step - loss: 0.3991 - acc: 0.8524 - val_loss: 0.3774 - val_acc: 0.8613\n",
      "Epoch 6/10\n",
      "46272/46272 [==============================] - 70s 2ms/step - loss: 0.3842 - acc: 0.8613 - val_loss: 0.3683 - val_acc: 0.8659\n",
      "Epoch 7/10\n",
      "46272/46272 [==============================] - 72s 2ms/step - loss: 0.3709 - acc: 0.8690 - val_loss: 0.3587 - val_acc: 0.8689\n",
      "Epoch 8/10\n",
      "46272/46272 [==============================] - 66s 1ms/step - loss: 0.3592 - acc: 0.8749 - val_loss: 0.3501 - val_acc: 0.8714\n",
      "Epoch 9/10\n",
      "46272/46272 [==============================] - 79s 2ms/step - loss: 0.3504 - acc: 0.8799 - val_loss: 0.3451 - val_acc: 0.8716\n",
      "Epoch 10/10\n",
      "46272/46272 [==============================] - 74s 2ms/step - loss: 0.3411 - acc: 0.8840 - val_loss: 0.3374 - val_acc: 0.8741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124f12ef0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_lstm.fit(seq_train, y_train, validation_split=0.2, epochs=10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_left_val = pipeline.transform(df_val['job_left'])\n",
    "X_right_val = pipeline.transform(df_val['job_right'])\n",
    "seq_val = [X_left_val, X_right_val]\n",
    "\n",
    "y_val = df_val['target'].values\n",
    "y_prob = siamese_lstm.predict(seq_val)\n",
    "y_pred = np.round(y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9411525077440928"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "roc_auc_score(y_val, y_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
